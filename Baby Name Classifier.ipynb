{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/codio/workspace/.guides/hf')\n",
    "from helper import *\n",
    "\n",
    "%matplotlib inline\n",
    "print('You\\'re running python %s' % sys.version.split(' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>The <code>hashfeatures</code> and <code>name2features</code> Functions</h3>\n",
    "<p>Below, the <code>hashfeatures</code> and <code>name2features</code> functions will take the plain text names and convert them to feature vectors so that you'll be able to work with the data effectively.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfeatures(baby, B, FIX):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        baby : a string representing the baby's name to be hashed\n",
    "        B: the number of dimensions to be in the feature vector\n",
    "        FIX: the number of chunks to extract and hash from each string\n",
    "    \n",
    "    Output:\n",
    "        v: a feature vector representing the input string\n",
    "    \"\"\"\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def name2features(filename, B=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<p>In the code cell above, <code>name2features</code> reads every name in the given file and converts it into a 128-dimensional feature vector by first assembling substrings (based on the parameter 'FIX'), then hashing these assembled substrings and modifying the feature vector index (the modulo of the number of dimensions) that corresponds to this hash value. </p> \n",
    "\n",
    "<p>Can you see how the feature vector for each name changes with different parameters? (Understanding how these features are constructed will help you later on in the challenge.)<br></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>The <code>genTrainFeatures</code> Function</h3>\n",
    "<p>We have provided you with a python function <code>genTrainFeatures</code>, which transforms the names into features and loads them into memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genTrainFeatures(dimension=128):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        dimension: desired dimension of the features\n",
    "    Output: \n",
    "        X: n feature vectors of dimensionality d (nxd)\n",
    "        Y: n labels (-1 = girl, +1 = boy) (n)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load in the data\n",
    "    Xgirls = name2features(\"girls.train\", B=dimension)\n",
    "    Xboys = name2features(\"boys.train\", B=dimension)\n",
    "    X = np.concatenate([Xgirls, Xboys])\n",
    "    \n",
    "    # Generate Labels\n",
    "    Y = np.concatenate([-np.ones(len(Xgirls)), np.ones(len(Xboys))])\n",
    "    \n",
    "    # shuffle data into random order\n",
    "    ii = np.random.permutation([i for i in range(len(Y))])\n",
    "    \n",
    "    return X[ii, :], Y[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<p>You can call the following command to return two vectors, one holding all the concatenated feature vectors and one holding the labels of all boys and girls names.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = genTrainFeatures(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2> The Na&iuml;ve Bayes Classifier </h2>\n",
       "\n",
       "<p> The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following cells will walk you through steps and ask you to finish the necessary functions in a pre-defined order. <strong>As a general rule, you should avoid tight loops at all costs.</strong></p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h2> The Na&iuml;ve Bayes Classifier </h2>\n",
    "\n",
    "<p> The Na&iuml;ve Bayes classifier is a linear classifier based on Bayes Rule. The following cells will walk you through steps and ask you to finish the necessary functions in a pre-defined order. <strong>As a general rule, you should avoid tight loops at all costs.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h3>Part One: Class Probability [Graded]</h3>\n",
       "<i>\n",
       "<p>Estimate the class probability $P(y)$ in \n",
       "<b><code>naivebayesPY</code></b>. This should return the probability that a sample in the training set is positive or negative, independent of its features.\n",
       "</p>\n",
       "</i>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<h3>Part One: Class Probability [Graded]</h3>\n",
    "<i>\n",
    "<p>Estimate the class probability $P(y)$ in \n",
    "<b><code>naivebayesPY</code></b>. This should return the probability that a sample in the training set is positive or negative, independent of its features.\n",
    "</p>\n",
    "</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayesPY(X, Y):\n",
    "    \"\"\"\n",
    "    naivebayesPY(Y) returns [pos,neg]\n",
    "\n",
    "    Computation of P(Y)\n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (nx1)\n",
    "\n",
    "    Output:\n",
    "        pos: probability p(y=1)\n",
    "        neg: probability p(y=-1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    Y = np.concatenate([Y, [-1,1]])\n",
    "    n = len(Y)\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    Y[Y ==-1 ] = 0\n",
    "    print(Y)\n",
    "    print(sum(Y)) #601\n",
    "    print(len(Y))\n",
    "    pos = sum(Y)/len(Y)\n",
    "    neg = 1-pos\n",
    "    print('pos', pos)\n",
    "    return [pos, neg]\n",
    "    #print(sum(Y)) # equals zero so we know there are the same number of 1s as -1s\n",
    "    #print(n) 1202\n",
    "    \n",
    "    \n",
    "    #return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    raise NotImplementedError()\n",
    "\n",
    "pos, neg = naivebayesPY(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following tests will check that the probabilities returned by your function sum to 1 (test1) and return the correct probabilities for a given set of input vectors (tests 2-4)\n",
    "\n",
    "# Check that probabilities sum to 1\n",
    "def naivebayesPY_test1():\n",
    "    pos, neg = naivebayesPY(X,Y)\n",
    "    return np.linalg.norm(pos + neg - 1) < 1e-5\n",
    "\n",
    "# Test the Naive Bayes PY function on a simple example\n",
    "def naivebayesPY_test2():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = .5, .5\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "# Test the Naive Bayes PY function on another example\n",
    "def naivebayesPY_test3():\n",
    "        x = np.array([[0,1,1,0,1],\n",
    "            [1,0,0,1,0],\n",
    "            [1,1,1,1,0],\n",
    "            [0,1,1,0,1],\n",
    "            [1,0,1,0,0],\n",
    "            [0,0,1,0,0],\n",
    "            [1,1,1,0,1]])    \n",
    "        y = np.array([1,-1, 1, 1,-1,-1, 1])\n",
    "        pos, neg = naivebayesPY(x,y)\n",
    "        pos0, neg0 = 5/9., 4/9.\n",
    "        test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "        return test < 1e-5\n",
    "\n",
    "# Tests plus-one smoothing\n",
    "def naivebayesPY_test4():\n",
    "    x = np.array([[0,1,1,0,1],[1,0,0,1,0]])    \n",
    "    y = np.array([1,1])\n",
    "    pos, neg = naivebayesPY(x,y)\n",
    "    pos0, neg0 = 3/4., 1/4.\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5    \n",
    "        \n",
    "    \n",
    "runtest(naivebayesPY_test1, 'naivebayesPY_test1')\n",
    "runtest(naivebayesPY_test2,'naivebayesPY_test2')\n",
    "runtest(naivebayesPY_test3,'naivebayesPY_test3')\n",
    "runtest(naivebayesPY_test4,'naivebayesPY_test4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>Part Two: Conditional Probability [Graded]</h3>\n",
    "<p>Estimate the conditional probabilities $P([\\mathbf{x}]_{\\alpha}|y)$ in \n",
    "<b><code>naivebayesPXY</code></b>. Notice that by construction, our features are binary categorical features. Use a <b>categorical</b> distribution as model and return the probability vectors for each feature being 1 given a class label.  Note that the result will be two vectors of length d (the number of features), where the values represent the probability that feature i is equal to 1.\n",
    "</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayesPXY(X,Y):\n",
    "    \"\"\"\n",
    "    naivebayesPXY(X, Y) returns [posprob,negprob]\n",
    "    \n",
    "    Input:\n",
    "        X : n input vectors of d dimensions (nxd)\n",
    "        Y : n labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        posprob: probability vector of p(x_alpha = 1|y=1)  (d)\n",
    "        negprob: probability vector of p(x_alpha = 1|y=-1) (d)\n",
    "    \"\"\"\n",
    "    \n",
    "    # add one positive and negative example to avoid division by zero (\"plus-one smoothing\")\n",
    "    n, d = X.shape\n",
    "    X = np.concatenate([X, np.ones((2,d)), np.zeros((2,d))])\n",
    "    Y = np.concatenate([Y, [-1,1,-1,1]])\n",
    "    n, d = X.shape\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    #take the Bayes algo iteratively on each element of a given dimension\n",
    "    #find the mean of each of the 1204 values in n???\n",
    "    #Remember each row is a unique item or instance\n",
    "    #the 128 features, d, represent the hashed apsects of the pre and suffix of a given name\n",
    "    #Remember we're mitigating the curse of dimentionality by assuming conditional independant features\n",
    "    #this means we want to take Bayes on each row. Then take the product of each of the individual probs\n",
    "    #meaning the probability that a baby name is male of female is the SUM of the probabilities of the each aspect (the Y vlue) being male or female??\n",
    "    #we take the exponentiated sum of logariths of each y term\n",
    "    #Note X[1,:] refers to all features (columns)\n",
    "    #Note X[:,1] gives us all rows in the first column\n",
    "    # Y has shape (1204,)\n",
    "    # X has shape(1204, 128)\n",
    "    \n",
    "    \n",
    "    #take the mean (probability)\n",
    "    #test = X[0][X==1]\n",
    "   # Xjprob = X[1:,]/len(d)\n",
    "    Xjmean = np.mean(X,0)\n",
    "    Xjmeanlog = np.log(Xjmean)\n",
    "    \n",
    "    \n",
    "    #classConditional = 2\n",
    "    \n",
    "    #Xjmeanlogsum = np.sum(Xjmeanlog)\n",
    "    \n",
    "    \n",
    "    #posprob= np.argmax(np.sum(np.log(classConditional), np.log(pos)))     #np.sum(np.log(pos))\n",
    "    posprob = np.mean(X[Y==1],0)\n",
    "    negprob = np.mean(X[Y==-1],0)\n",
    "    \n",
    "    \n",
    "\n",
    "    return posprob, negprob\n",
    "    #return [posprob, negprob]\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "posprob, negprob = naivebayesPXY(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of naivebayesPXY returns the same posterior probabilities as the correct implementation, in the correct dimensions\n",
    "\n",
    "# test a simple toy example with two points (one positive, one negative)\n",
    "def naivebayesPXY_test1():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    pos, neg = naivebayesPXY(x,y)\n",
    "    pos0, neg0 = naivebayesPXY_grader(x,y)\n",
    "    test = np.linalg.norm(pos - pos0) + np.linalg.norm(neg - neg0)\n",
    "    return test < 1e-5\n",
    "\n",
    "# test the probabilities P(X|Y=+1)\n",
    "def naivebayesPXY_test2():\n",
    "    pos, neg = naivebayesPXY(X,Y)\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    test = np.linalg.norm(pos - posprobXY) \n",
    "    return test < 1e-5\n",
    "\n",
    "# test the probabilities P(X|Y=-1)\n",
    "def naivebayesPXY_test3():\n",
    "    pos, neg = naivebayesPXY(X,Y)\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    test = np.linalg.norm(neg - negprobXY)\n",
    "    return test < 1e-5\n",
    "\n",
    "\n",
    "# Check that the dimensions of the posterior probabilities are correct\n",
    "def naivebayesPXY_test4():\n",
    "    pos, neg = naivebayesPXY(X,Y)\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    return pos.shape == posprobXY.shape and neg.shape == negprobXY.shape\n",
    "\n",
    "runtest(naivebayesPXY_test1,'naivebayesPXY_test1')\n",
    "runtest(naivebayesPXY_test2,'naivebayesPXY_test2')\n",
    "runtest(naivebayesPXY_test3,'naivebayesPXY_test3')\n",
    "runtest(naivebayesPXY_test4,'naivebayesPXY_test4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>Part Three: Log Lokelihood [Graded]</h3>\n",
    "\n",
    "<i>\n",
    "<p>Calculate the log likelihood $\\log P(\\mathbf{x}|y)$ for each point in X_test in \n",
    "<b><code>loglikelihood</code></b> and label Y_test. Recall that the likelihood is given by the product of the conditional probabilities of each feature and that $\\log(ab) = \\log a + \\log b$.\n",
    "</p> \n",
    "<i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(posprob, negprob, X_test, Y_test):\n",
    "    \"\"\"\n",
    "    loglikelihood(posprob, negprob, X_test, Y_test) returns loglikelihood of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "        Y_test : labels (-1 or +1) (n)\n",
    "    \n",
    "    Output:\n",
    "        loglikelihood of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    n, d = X_test.shape\n",
    "    loglikelihood = np.zeros(n)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #posprob, negprob = naivebayesPXY(X,Y)\n",
    "    plog = (X_test[Y_test==1]@np.log(posprob)) +((1- X_test[Y_test==1])@np.log(1-posprob))\n",
    "    nlog = (X_test[Y_test==-1]@np.log(negprob)) + ((1-X_test[Y_test==-1])@np.log(1-negprob))\n",
    "    #print('plog', plog)\n",
    "    #print('nlog', nlog)\n",
    "    \n",
    "    #apply\n",
    "    loglikelihood[Y_test==1] = plog\n",
    "    loglikelihood[Y_test==-1] = nlog\n",
    "    \n",
    "    return loglikelihood\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "\n",
    "# compute the loglikelihood of the training set\n",
    "posprob, negprob = naivebayesPXY(X,Y)\n",
    "loglikelihood(posprob,negprob,X,Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of loglikelihood returns the same values as the correct implementation for three different datasets\n",
    "\n",
    "X, Y = genTrainFeatures(128)\n",
    "posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "\n",
    "# test if the log likelihood of the training data are all negative\n",
    "def loglikelihood_testneg():\n",
    "    ll=loglikelihood(posprob,negprob,X,Y);\n",
    "    return all(ll<0)\n",
    "\n",
    "# test if the log likelihood of the training data matches the solution\n",
    "def loglikelihood_test0():\n",
    "    ll=loglikelihood(posprob,negprob,X,Y);\n",
    "    llgrader=loglikelihood_grader(posprob,negprob,X,Y);\n",
    "    return np.linalg.norm(ll-llgrader)<1e-5\n",
    "\n",
    "# test if the log likelihood of the training data matches the solution\n",
    "# (positive points only)\n",
    "def loglikelihood_test0a():\n",
    "    ll=loglikelihood(posprob,negprob,X,Y);\n",
    "    llgrader=loglikelihood_grader(posprob,negprob,X,Y);\n",
    "    return np.linalg.norm(ll[Y==1]-llgrader[Y==1])<1e-5\n",
    "\n",
    "# test if the log likelihood of the training data matches the solution\n",
    "# (negative points only)\n",
    "def loglikelihood_test0b():\n",
    "    ll=loglikelihood(posprob,negprob,X,Y);\n",
    "    llgrader=loglikelihood_grader(posprob,negprob,X,Y);\n",
    "    return np.linalg.norm(ll[Y==-1]-llgrader[Y==-1])<1e-5\n",
    "\n",
    "\n",
    "# little toy example with two data points (1 positive, 1 negative)\n",
    "def loglikelihood_test1():\n",
    "    x = np.array([[0,1],[1,0]])\n",
    "    y = np.array([-1,1])\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    loglike = loglikelihood(posprobXY[:2], negprobXY[:2], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:2], negprobXY[:2], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "# little toy example with four data points (2 positive, 2 negative)\n",
    "def loglikelihood_test2():\n",
    "    x = np.array([[1,0,1,0,1,1], \n",
    "        [0,0,1,0,1,1], \n",
    "        [1,0,0,1,1,1], \n",
    "        [1,1,0,0,1,1]])\n",
    "    y = np.array([-1,1,1,-1])\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    loglike = loglikelihood(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "\n",
    "# one more toy example with 5 positive and 2 negative points\n",
    "def loglikelihood_test3():\n",
    "    x = np.array([[1,1,1,1,1,1], \n",
    "        [0,0,1,0,0,0], \n",
    "        [1,1,0,1,1,1], \n",
    "        [0,1,0,0,0,1], \n",
    "        [0,1,1,0,1,1], \n",
    "        [1,0,0,0,0,1], \n",
    "        [0,1,1,0,1,1]])\n",
    "    y = np.array([1, 1, 1 ,1,-1,-1, 1])\n",
    "    posprobXY, negprobXY = naivebayesPXY_grader(X, Y)\n",
    "    loglike = loglikelihood(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    loglike0 = loglikelihood_grader(posprobXY[:6], negprobXY[:6], x, y)\n",
    "    test = np.linalg.norm(loglike - loglike0)\n",
    "    return test < 1e-5\n",
    "\n",
    "\n",
    "runtest(loglikelihood_testneg, 'loglikelihood_testneg (all log likelihoods must be negative)')\n",
    "runtest(loglikelihood_test0, 'loglikelihood_test0 (training data)')\n",
    "runtest(loglikelihood_test0a, 'loglikelihood_test0a (positive points)')\n",
    "runtest(loglikelihood_test0b, 'loglikelihood_test0b (negative points)')\n",
    "runtest(loglikelihood_test1, 'loglikelihood_test1')\n",
    "runtest(loglikelihood_test2, 'loglikelihood_test2')\n",
    "runtest(loglikelihood_test3, 'loglikelihood_test3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h3>Part Four: Naive Bayes Prediction [Graded]</h3>\n",
    "\n",
    "\n",
    "<p>Observe that for a test point $\\mathbf{x}_{test}$, we should classify it as positive if the log ratio $\\log\\left(\\frac{P(y=1 | \\mathbf{x} = \\mathbf{x}_{test})}{P(y=-1|\\mathbf{x} = \\mathbf{x}_{test})}\\right) > 0$ and negative otherwise. Implement the <b><code>naivebayes_pred</code></b> by first calculating the log ratio $\\log\\left(\\frac{P(y=1 | \\mathbf{x} = \\mathbf{x}_{test})}{P(y=-1|\\mathbf{x} = \\mathbf{x}_{test})}\\right)$ for each test point in $\\mathbf{x}_{test}$ using Bayes' rule and predict the label of the test points by looking at the log ratio. When calculating the log likelihood, think carefully how you can use the fact $\\log \\left(\\frac{a}{b}\\right) = \\log{a} - \\log{b}$ to simplify your calculations.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naivebayes_pred(pos, neg, posprob, negprob, X_test):\n",
    "    \"\"\"\n",
    "    naivebayes_pred(pos, neg, posprob, negprob, X_test) returns the prediction of each point in X_test\n",
    "    \n",
    "    Input:\n",
    "        pos: class probability for the negative class\n",
    "        neg: class probability for the positive class\n",
    "        posprob: conditional probabilities for the positive class (d)\n",
    "        negprob: conditional probabilities for the negative class (d)\n",
    "        X_test : features (nxd)\n",
    "    \n",
    "    Output:\n",
    "        prediction of each point in X_test (n)\n",
    "    \"\"\"\n",
    "    n, d = X_test.shape\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    ones = np.ones(n)\n",
    "    negones = -np.ones(n)\n",
    "    \n",
    "    boyslikelihood = loglikelihood(posprob, negprob,X_test, ones)\n",
    "    girlslikelihood = loglikelihood(posprob, negprob,X_test, negones)\n",
    "    \n",
    "    \n",
    "    #likelipred = boyslikelihood + np.log(pos) - girlslikelihood - np.log(neg)\n",
    "    likelipred = boyslikelihood - girlslikelihood\n",
    "    likelipred2 = np.where(likelipred > 0 , 1 , -1)\n",
    "    return likelipred2\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following tests check that your implementation of naivebayes_pred returns only 1s and -1s (test 1), and that it returns the same predicted values as the correct implementation for three different datasets (tests 2-4)\n",
    "\n",
    "X,Y = genTrainFeatures_grader(128)\n",
    "posY, negY = naivebayesPY_grader(X, Y)\n",
    "\n",
    "# check whether the predictions are +1 or neg 1\n",
    "def naivebayes_pred_test1():\n",
    "    preds = naivebayes_pred(posY, negY, posprobXY, negprobXY, X)\n",
    "    return np.all(np.logical_or(preds == -1 , preds == 1))\n",
    "\n",
    "def naivebayes_pred_test2():\n",
    "    naivebayesPXY_grader(X, Y)\n",
    "    x_test = np.array([[0,1],[1,0]])\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:2], negprobXY[:2], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:2], negprobXY[:2], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "def naivebayes_pred_test3():\n",
    "    x_test = np.array([[1,0,1,0,1,1], \n",
    "        [0,0,1,0,1,1], \n",
    "        [1,0,0,1,1,1], \n",
    "        [1,1,0,0,1,1]])\n",
    "    naivebayesPXY_grader(X, Y)\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "def naivebayes_pred_test4():\n",
    "    x_test = np.array([[1,1,1,1,1,1], \n",
    "        [0,0,1,0,0,0], \n",
    "        [1,1,0,1,1,1], \n",
    "        [0,1,0,0,0,1], \n",
    "        [0,1,1,0,1,1], \n",
    "        [1,0,0,0,0,1], \n",
    "        [0,1,1,0,1,1]])\n",
    "    naivebayesPXY_grader(X, Y)\n",
    "    preds = naivebayes_pred_grader(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    student_preds = naivebayes_pred(posY, negY, posprobXY[:6], negprobXY[:6], x_test)\n",
    "    acc = analyze_grader(\"acc\", preds, student_preds)\n",
    "    return np.abs(acc - 1) < 1e-5\n",
    "\n",
    "runtest(naivebayes_pred_test1, 'naivebayes_pred_test1')\n",
    "runtest(naivebayes_pred_test2, 'naivebayes_pred_test2')\n",
    "runtest(naivebayes_pred_test3, 'naivebayes_pred_test3')\n",
    "runtest(naivebayes_pred_test4, 'naivebayes_pred_test4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You can now test your code with the following interactive name classification script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = 128\n",
    "print('Loading data ...')\n",
    "X,Y = genTrainFeatures(DIMS)\n",
    "print('Training classifier ...')\n",
    "pos, neg = naivebayesPY(X, Y)\n",
    "posprob, negprob = naivebayesPXY(X, Y)\n",
    "error = np.mean(naivebayes_pred(pos, neg, posprob, negprob, X) != Y)\n",
    "print('Training error: %.2f%%' % (100 * error))\n",
    "\n",
    "while True:\n",
    "    print('Please enter a baby name>')\n",
    "    yourname = input()\n",
    "    if len(yourname) < 1:\n",
    "        break\n",
    "    xtest = name2features(yourname,B=DIMS,LoadFile=False)\n",
    "    pred = naivebayes_pred(pos, neg, posprob, negprob, xtest)\n",
    "    if pred > 0:\n",
    "        print(\"%s, I am sure you are a baby boy.\\n\" % yourname)\n",
    "    else:\n",
    "        print(\"%s, I am sure you are a baby girl.\\n\" % yourname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<h2> Challenge: Feature Extraction</h2>\n",
    "\n",
    "<p>Let's test how well your Na&iuml;ve Bayes classifier performs on a secret test set. If you want to improve your classifier modify <code>name2features2</code> below.   The automatic reader will use your Python script to extract features and train your classifier on the same names training set by calling the function with only one argument--the name of a file containing a list of names.  The given implementation is the same as the given <code>name2features</code> above.\n",
    "</p>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashfeatures(baby, B, FIX):\n",
    "    v = np.zeros(B)\n",
    "    for m in range(FIX):\n",
    "        featurestring = \"prefix\" + baby[:m]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "        featurestring = \"suffix\" + baby[-1*m:]\n",
    "        v[hash(featurestring) % B] = 1\n",
    "    return v\n",
    "\n",
    "def name2features2(filename, B=128, FIX=3, LoadFile=True):\n",
    "    \"\"\"\n",
    "    Output:\n",
    "        X : n feature vectors of dimension B, (nxB)\n",
    "    \"\"\"\n",
    "    # read in baby names\n",
    "    if LoadFile:\n",
    "        with open(filename, 'r') as f:\n",
    "            babynames = [x.rstrip() for x in f.readlines() if len(x) > 0]\n",
    "    else:\n",
    "        babynames = filename.split('\\n')\n",
    "    n = len(babynames)\n",
    "    X = np.zeros((n, B))\n",
    "    for i in range(n):\n",
    "        X[i,:] = hashfeatures(babynames[i], B, FIX=5)\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autograder test cell- competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Hint: You should be able to get >80% accuracy just by changing some of the default hyperparameters in the function argument.  If you'd like to try something more sophisticated, you can add to `name2features2`)\n",
    "\n",
    "<h4>Credits</h4>\n",
    " The name classification idea originates from <a href=\"http://nickm.com\">Nick Montfort</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
